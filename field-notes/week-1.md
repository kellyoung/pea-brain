# week-1 learnings

I started dabbling in techno-optimism in the past few weeks, which has opened me up to exploring AI a little bit more. In my exploration I kept thinking about how much I value my personal journey of thought and how much better computers are at remembering things. From what I learned so far, it seemed really possible to use AI to build a digital representation of my mind. I wanted to build something where it would be easy to store my thoughts and recall them. I started thinking about LLM's (large language models) as a representation of a large collective mind and in that vein,  wondered if I could build an LLM of my own mind. Since my mind is so tiny in comparison, I wanted to call this LLM "pea-brain" and wanted to build tools for others to also create their own pea brains. The name "pea-brain" was so compelling, I decided I must see it to fruition!

I learned about [Inflection 2.5](https://inflection.ai/inflection-2-5) an LLM worth a gajillion dollars because it's much tinier than other LLM's like GPT-4 but still really performant . This got me thinking about how tiny could an LLM be, and I thought it could get really tiny if I only wanted it to know the contents of my own mind. However, since there aren't _that_ many thoughts going on in my head, I thought if it only had my thinking, it would not have enough content to learn the English language, so somehow I would need to feed in the language-comprehension aspects of the LLM into pea-brain. Since, then I've learned instead of "feeding language comprehension to your tiny pea-brain" you would instead use a technique called Retrieval Augmented Generation (RAG) to give an LLM more specific context of where to retrieve information from. Turns out RAG is a pretty big deal, and companies like Quivr and LlamaIndex are both doing some major and awesome work in this field. Quivr essentially built what I want to build but ✨yassified✨ and on a larger enterprise scale and LlamaIndex has the tooling built out that I'll probably use for pea-brain. I'm building for fun and learning, so even though I'm no Christopher Columbus in this situation, it's still a worthy pursuit.

How I thought `pea-brain` would work (**WARNING: THIS IS NOT HOW IT WORKS**!):

![](/assets/images/proto-pea-brain-2-20240405112843470-4025x1812.png)

Now that I decided RAG was the way to go, the next piece of the puzzle to figure out was how to give an LLM the content of my mind. From poking around Quivr, I saw how you can customize your "brain" by giving "prompt instructions" for how the LLM should "think of itself". An example: "You are an academic researcher. Conduct research, analyze data, and publish findings in academic journals and conferences. Maintain professionalism and accuracy, contribute innovative knowledge, and communicate findings effectively to peers and the public. Additionally, ensure to properly cite all references and sources used in your research to acknowledge original authors and avoid plagiarism." That gave me a clue into how the most rudimentary implementation of `pea-brain` would work: I would pass in my context as a large body of text and ask the LLM to utilize that context when answering the question. An example prompt would look like: "This is the content of my journal: <REALLY LARGE BODY OF TEXT>. In my journal, can you summarize my thoughts about dog training?"

However, my time Leetcoding really had me questioning performance and feeling really unsettled about the performance of needing the LLM  to ingest the body of text for every single interaction. What if I was a prolific journalist? What if I keep writing in my journal, and it only continues to grow in size? This lead me to learn about using a vector store for `pea-brain`.

This is my elementary understanding of how vector stores and embeddings work. My partner illustrated to me in the terms of a map, which I found really helpful to understand how the performance improves. I'll try my best to describe what I learned:

![](/assets/images/image-20240405225731045-4438x3193.png)

Similar to the map concept, how LLM's communicate with a vector store is to create an embedding (a numerical representation) of the query, and then find embeddings in the Vector Store that are in proximity to the embedding of the query. I think what's most mind-blowing to me about this is the ability to determine proximity based on meaning and not based on literal similarity like "token" and "spoken" sharing the common characters of "oken". When "word2vec" was published, my partner was so hyped about it that the name stuck with me. Turns out, its findings kind of form the basis of using vector embeddings for understanding language. As an aside, I find it strangely touching to see how many different discoveries and people it takes to advance technology and how only in retrospect can you really understand their significance. It makes me want to continue stepping away from the "superhero mentality" (the solo founder narrative) and truly embody the collaborative nature of the world I want to live in.

With a vector store, performance improves because instead of looking at the entire body of writings every single query, first you query the vector store for relevant data before prompting the LLM with the relevant context. This is how I anticipate `pea-brain` will interact with the LLM and vector store:

![](/assets/images/pea-brain-arch-20240405135939733-4843x2359.png)

These are things I'm planning to work on for `pea-brain`:

- **recording tools**\- my job becomes documentarian, I must gather content for my `pea-brain` that it can ingest. Though writing things down is an extra step, so is filming a DIML on TikTok. And, I'd rather have to type some notes down than have a chip implanted in my brain that has access to all my thoughts. I plan to create tools to gather-content from pure digital notes (right now I'm writing files to Github with a nice sparkly interface thanks to [Bangle](https://bangle.io/) (so much faster than Notion!!!!!). I also want to gather content from annotations I make on web articles (using [Diigo](https://www.diigo.com/index), which is not aesthetically pleasing to me but useful) and from notes I take on the Kindle app. I plan to build tooling to marshal and send the data to be ingested into a Vector Store. In the future, I might want to explore building my own note keeper app that can automatically sync with `pea-brain`.

- **data ingestion/storage** - as mentioned above, I want my "thoughts in text" to end up in a vector store. Therefore, I'll need to build a process that can continuously turn my writings into embeddings and feed them to the vector store.

- **LLM integration**\- turns out, there aren't that many API's out there that let you programmatically interact with an LLM. OpenAI has the most established one right now, but it's going to cost you! I really want to explore my open source options and learn to run a local LLM. In addition, I'll need to turn my queries into embeddings to pass to the vector store. Maybe I'll also be able to build some tooling around running an LLM locally and interacting with it programatically. For now, I think I can rely pretty heavily on LlamaIndex to build out the functionality.

- **LLM user interface** - this is something I'm super interested in because I think it's really the wild west out here. Currently, I think a lot of interaction with an LLM is through a chat or a query, but I'm really curious about if there's any other interfaces that would make sense. I thought about running an open-ended design workshop for myself to try to think of novel ways that I can interact with my `pea-brain`.

If you read this, you're probably my family, so I love you forever! Thanks for reading, and I hope you'll stay tuned!